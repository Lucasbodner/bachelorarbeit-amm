# ğŸ§  Bachelorarbeit â€“ AMM Mobile App

This project is part of my Bachelor's thesis at **DFKI** and **HTW Saar**.  
It aims to develop a privacy-preserving mobile/web application that integrates an **ğŸ§ Artificial Mental Model (AMM)** to support healthcare professionals with local inference and offline functionality.

---

## ğŸ“š Table of Contents

- [Project Description](#-project-description)
- [Project Structure](#-project-structure)
- [About the AMM](#-about-the-amm)
- [Privacy & Offline Functionality](#-privacy--offline-functionality)
- [Installation & Usage](#-installation--usage)
- [Current Status](#-current-status)
- [Author](#-author)
- [Timeline](#-timeline)

---

## ğŸ“œ Project Description

The goal is to create a local, lightweight mobile/web application that runs a quantized Artificial Mental Model directly on a user device (e.g. smartphone or tablet).  
The interface is built with **Streamlit** (as a web prototype) and will later be adapted to better fit mobile platforms.

---

## ğŸ“ Project Structure

```
bachelorarbeit-amm/
â”œâ”€â”€ app.py                   # Main Streamlit application
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ README.md                # Project description

â”œâ”€â”€ model/                   # Quantized AMM model (.gguf)
â”‚   â””â”€â”€ Llama-3.2-1B-merged-lora-q4_k.gguf
â”œâ”€â”€ bin/                     # llama-cli binary
â”‚   â””â”€â”€ llama-cli
â”œâ”€â”€ utils/                   # Helper functions (e.g., inference logic)
â”œâ”€â”€ data/                    # Saved conversations
â”‚   â””â”€â”€ conversations.txt
â”œâ”€â”€ doc/                     # Notes, articles, diagrams, screenshots
```

---

## ğŸ§  About the AMM

This application uses a quantized **Artificial Mental Model (AMM)** based on a local large language model (LLM), stored as a `.gguf` file.  
The goal is to assist medical professionals with decision-making, especially in remote or privacy-sensitive scenarios.

- The model runs fully **offline**
- Built on **llama-cpp** and executed via a local `llama-cli` binary
- The interface allows users to input questions and receive generated responses
- Designed with a focus on healthcare-related use cases

---

## ğŸ” Privacy & Offline Functionality

- All inference and data storage happens **locally** on the device
- No internet access is required
- All conversations are saved in `data/conversations.txt`
- Optional: Patient data will later be **encrypted** (e.g., via Fernet)

---

## âš™ï¸ Installation & Usage

### 1. Clone the repository

```bash
git clone https://github.com/lucasbodner/bachelorarbeit-amm.git
cd bachelorarbeit-amm
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Make sure the llama-cli binary is available

The binary should be located in `bin/llama-cli` and be executable. If not, compile it or download from a release source.

### 4. Start the prototype

```bash
streamlit run app.py
```

The app will launch at `http://localhost:8501`.  
You can type in a question and get a response from the local `.gguf` model.

---

## âœ… Current Status

- âœ… Initial setup complete
- âœ… Streamlit prototype working locally with `.gguf` model
- âœ… Responses saved in `data/conversations.txt`
- ğŸ› ï¸ Model fine-tuning & UI improvements in progress
- ğŸ”’ Local encryption planned for August

---

## ğŸ‘¨â€ğŸ’¼ Author

**Lucas Bodner**  
Bachelor student at **HTW Saar** / Intern at **DFKI**  
ğŸ“§ lucas.bodner@htwsaar.de

---

## â³ Timeline

| Week | Task                                    | Status     |
|------|-----------------------------------------|------------|
| W1â€“W2 | Model selection & integration           | âœ… Done     |
| W3   | Working local prototype (CLI + UI)      | âœ… Done     |
| W4   | Response filtering + data storage       | âœ… Done     |
| W5   | UI polish, README, folder cleanup       | ğŸŸ¡ In Progress |
| W6+  | Add encryption, refine design, testing  | ğŸ–’ Planned  |

