import streamlit as st
import subprocess
import os
import re

# --- Configuration de la page ---
st.set_page_config(page_title="AMM Mobile App ‚Äì Local CLI", page_icon="üß†")
st.title("üß† AMM Mobile App ‚Äì Streamlit Prototype")
st.markdown("Ask your local `.gguf` model powered by llama-cli")

# --- Zone de texte utilisateur ---
user_input = st.text_area("Your question:", value="How can AI support physiotherapy?", height=100)

# --- Fonction de g√©n√©ration de r√©ponse ---
def generate_response(prompt):
    try:
        result = subprocess.run(
            [
                "./bin/llama-cli",
                "-m", "model/Llama-3.2-1B-merged-lora-q4_k.gguf",
                "-p", prompt,
                "-n", "200"
            ],
            capture_output=True,
            text=True
        )

        # D√©coupage ligne par ligne
        output_lines = result.stdout.strip().split("\n")

        # Filtrage pour enlever les logs techniques
        filtered_lines = [
            line for line in output_lines
            if not line.startswith(("llama_", "ggml_", "build:", "main:", "load:", "print_info:",
                                    "sampler", "generate:", "llama_perf", "system_info:", "ggml_metal_"))
        ]

        # Join et nettoyage texte
        raw_response = "\n".join(filtered_lines).strip()

        # Nettoyage : suppression du prompt r√©p√©t√© au d√©but
        cleaned_response = re.sub(rf"^{re.escape(prompt)}[\s:]*", "", raw_response, flags=re.IGNORECASE)

        # Nettoyage optionnel : suppression des phrases g√©n√©riques inutiles
        cleaned_response = re.sub(r"(?i)^As (a|part).*?\. ?", "", cleaned_response).strip()

        return cleaned_response if cleaned_response else "‚ö†Ô∏è No response generated."

    except Exception as e:
        return f"‚ùå Error during model execution:\n{e}"

# --- Bouton de g√©n√©ration ---
if st.button("Generate response"):
    if user_input.strip():
        with st.spinner("Generating response..."):
            response = generate_response(user_input)

            # Affichage de la r√©ponse
            st.success("Response generated!")
            st.markdown("### üí¨ Response:")
            st.write(response)

            # Sauvegarde dans un fichier texte
            os.makedirs("data", exist_ok=True)
            with open("data/conversations.txt", "a") as f:
                f.write(f"Q: {user_input.strip()}\nA: {response}\n---\n")

    else:
        st.warning("Please enter a question first.")
